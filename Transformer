import torch
from torch import nn
import torch.nn.functional as F
import math

# 生成测试数据
# batch,time,dimension
# time指的是多少个词
# 512就是embedding后要映射的词向量长度
X = torch.randn(128, 64, 512)

d_model = 512  # 词向量长度，每一个词都由一个长度为 512 的向量表示
n_head = 8  # 8头


# 多头注意力
class multi_head_attention(nn.Module):
    # 构造函数，两个参数
    def __init__(self, d_model, n_head):
        super().__init__()

        self.n_head = n_head
        self.d_model = d_model
        # 这四个权重是层对象，里面装着权重矩阵，但它本身不是矩阵。
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        # 因为是多头注意力，最后需要用一个w_combine矩阵进行线性映射
        self.w_combine = nn.Linear(d_model, d_model)
        self.softmax = nn.Softmax(dim=-1)

    # 输入的qkv并不是真的qkv，而是生成qkv的原材料，如果是自注意力，那么这三个值就是一样的
    def forward(self, q, k, v, mask=None):
        batch, time, dimension = q.shape
        # 真正每个头的维度，要将总的除以8
        n_d = self.d_model // self.n_head
        # 这里才是真正的qkv
        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)

        # 维度划分
        # 1. view: 把最后一维切开
        # 原状: [batch, time, d_model] -> [128, 64, 512]
        # 变形: [batch, time, n_head, head_dim] -> [128, 64, 8, 64]
        # 2. permute: 交换维度
        # 原状: [batch, time, n_head, head_dim] (0, 1, 2, 3)
        # 目标: [batch, n_head, time, head_dim] (0, 2, 1, 3)
        # time和head_dim才是在计算时会用到的
        q = q.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)
        k = k.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)
        v = v.view(batch, time, self.n_head, n_d).permute(0, 2, 1, 3)

        # q×k计算score,然后要将score经过softmax
        # 对k的[time,n_d]进行转置成[n_d,time]
        # #最后矩阵乘法后的score为[time,n_d]@[n_d,time]=[time,time]
        score = q @ k.transpose(2, 3) / math.sqrt(n_d)
        # 注意如果是encoder是进行self attention的，不需要进行mask
        if mask is not None:
            # mask = torch.tril(torch.ones(time, time, dtype=bool))
            # 把0转化为-无穷
            score = score.masked_fill(mask == 0, float('-inf'))

        score = self.softmax(score)
        # [time,time]@[time,n_d] = [time,n_d]
        score = score @ v

        # 维度恢复,选择+把n_head和n_d乘起来恢复成dimension
        score = score.permute(0, 2, 1, 3).contiguous().view(batch, time, dimension)

        # 最后乘上w_combine[d_model,d_model]
        out_put = self.w_combine(score)
        return out_put


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        # 初始化一个 [max_len, d_model] 的矩阵
        pe = torch.zeros(max_len, d_model)

        # 生成位置索引 [0, 1, ... max_len-1] -> [max_len, 1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)

        # 计算分母 div_term: 10000^(2i/d_model)
        # 在对数空间计算更数值稳定: exp( -2i * ln(10000) / d_model )
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        # 偶数下标用 sin
        pe[:, 0::2] = torch.sin(position * div_term)
        # 奇数下标用 cos
        pe[:, 1::2] = torch.cos(position * div_term)

        # 增加一个维度以适配 batch: [1, max_len, d_model]
        pe = pe.unsqueeze(0)

        # 注册为 buffer (不是参数，不参与梯度更新，但会随模型保存状态)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]

        # 取出对应长度的位置编码并相加
        # 这里不需要 .to(device)，因为 register_buffer 会自动处理设备问题
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


class TransformerEmbedding(nn.Module):
    """
    整合 Token Embedding 和 Positional Encoding
    """

    def __init__(self, vocab_size, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.tok_emb = nn.Embedding(vocab_size, d_model, padding_idx=0)  # 假设0是padding
        self.pos_emb = PositionalEncoding(d_model, dropout, max_len)

    def forward(self, x):
        # x: [batch_size, seq_len] (输入的单词索引)
        tok_emb = self.tok_emb(x)  # -> [batch, seq_len, d_model]
        out = self.pos_emb(tok_emb)
        return out


# LayerNorm 对通道c（也就是最后一个维度进行归一化）
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-10):
        super().__init__()
        self.eps = eps  #超参数
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))

    def forward(self, x):
        val = x.var(-1, unbias=False, keepdim=True)
        mean = x.mean(-1, keepdim=True)
        out = (x - mean) / (torch.sqrt(val + self.eps))
        out = self.gamma * out + self.beta
        return out


# FFN
class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, hidden, dropout=0.1):
        super().__init__()
        self.fc1 = nn.Linear(d_model, hidden)
        self.fc2 = nn.Linear(hidden, d_model)
        dropout = nn.Dropout(dropout)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x


# Encoder Layer
class EncoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, n_head, dropout):
        super().__init__()
        self.attention = multi_head_attention(d_model, n_head)
        self.norm1 = LayerNorm(d_model)
        self.drop1 = nn.Dropout(dropout)

        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, dropout)
        self.norm2 = LayerNorm(d_model)  # layernorm是包含可学习参数的，得写两个
        self.drop2 = nn.Dropout(dropout)

    def forward(self, x):
        res = x
        x = self.attention(x, x, x, mask=None)
        x = self.drop1(x)
        x = self.norm1(x + res)  # add+norm层

        res = x
        x = self.ffn(x)
        x = self.drop2(x)
        x = self.norm2(x + res)
        return x


# DecoderLayer
class DecoderLayer(nn.Module):
    def __init__(self, d_model, ffn_hidden, n_head, dropout):
        super().__init__()
        self.attention1 = multi_head_attention(d_model, n_head)
        self.norm1 = LayerNorm(d_model)
        self.drop1 = nn.Dropout(dropout)

        self.cross_attention = multi_head_attention(d_model, n_head)
        self.norm2 = LayerNorm(d_model)
        self.drop2 = nn.Dropout(dropout)

        self.ffn = PositionwiseFeedForward(d_model, ffn_hidden, dropout)
        self.norm3 = LayerNorm(d_model)
        self.drop3 = nn.Dropout(dropout)

    def forward(self, enx, dex, tgt_mask, memory_mask):
        res = dex
        # masked attention
        dex = self.attention1(dex, dex, dex, mask=tgt_mask)
        dex = self.drop1(dex)
        dex = self.norm1(dex + res)

        # cross attention
        res = dex
        dex = self.cross_attention(dex, enx, enx, mask=memory_mask)
        dex = self.drop2(dex)
        dex = self.norm2(dex + res)

        # ffn
        res = dex
        dex = self.ffn(dex)
        dex = self.drop3(dex)
        dex = self.norm3(dex + res)

        return dex


if __name__ == '__main__':
    vocab_size = 1000
    d_model = 512
    max_len = 64

    # 1. 实例化 Embedding 层
    embedding_layer = TransformerEmbedding(vocab_size, d_model, max_len=100)

    # 2. 模拟输入数据: [Batch=2, Seq_Len=10] (全是单词ID)
    input_ids = torch.randint(0, vocab_size, (2, 10))

    # 3. 前向传播
    emb_out = embedding_layer(input_ids)

    print("输入 ID 形状:", input_ids.shape)  # [2, 10]
    print("Embedding 输出形状:", emb_out.shape)  # [2, 10, 512]

    # 接下来的 Encoder
    encoder_layer = EncoderLayer(d_model, ffn_hidden=2048, n_head=8, dropout=0.1)
    enc_out = encoder_layer(emb_out)
    print("Encoder 输出形状:", enc_out.shape)
